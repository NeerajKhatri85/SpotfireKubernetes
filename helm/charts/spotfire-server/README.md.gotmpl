{{ template "chart.header" . }}

{{ template "chart.deprecationWarning" . }}

{{ template "chart.badgesSection" . }}

{{ template "chart.description" . }}

{{ template "chart.homepageLine" . }}

{{ template "chart.maintainersSection" . }}

{{ template "chart.sourcesSection" . }}

{{ template "chart.requirementsSection" . }}

## Overview

This chart deploys the [TIBCO Spotfire® Server](https://docs.tibco.com/pub/spotfire_server/latest/doc/html/TIB_sfire_server_tsas_admin_help/server/topics/introduction_to_the_tibco_spotfire_environment.html) on a [Kubernetes](http://kubernetes.io/) cluster using the [Helm](https://helm.sh/) package manager.

Using this chart, you can also deploy the following:
- The required Spotfire Server database schemas on a supported database server (for example, Postgres).
- A reverse proxy ([HAProxy](https://www.haproxy.org/)) for accessing the Spotfire Server cluster service, with session affinity for external HTTP access.
- An ([Ingress](https://kubernetes.io/docs/concepts/services-networking/ingress/)) with routing rules for accessing the configured reverse proxy.
- Shared storage locations ([Persistent Volumes](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)) for the Spotfire library import and export, custom jars, deployment packages, and other features.

The Spotfire Server pod includes:
- A [Fluent Bit](https://fluentbit.io/) sidecar container for log forwarding.
- Service annotations for [Prometheus](https://prometheus.io/) scrapers. The Prometheus server discovers the service endpoint using these specifications and scrapes metrics from the exporter.

This chart is tested to work with [NGINX Ingress Controller](https://github.com/kubernetes/ingress-nginx), [Elasticsearch](https://www.elastic.co/elasticsearch/) and [Prometheus](https://prometheus.io/).

## Prerequisites

- Familiarity with kubernetes, containers and helm concepts and usage
- Helm 3+
- Ingress controller (optional)
- PV (Persistent Volume) provisioner support in the underlying infrastructure (optional)
- A supported database server for use as the Spotfire database (For information on supported databases, see [Spotfire Server requirements](https://docs.tibco.com/pub/spotfire/general/sr/sr/topics/system_requirements_for_spotfire_products.html).)

**Note**: You can install the database server in the same kubernetes cluster or on premises. Alternatively, you can use a cloud database service.


## Usage

### Installing

To install the chart with the release name `my-release` and the values from the file `my-values`:
```bash
helm install my-release -f my-values.yml .
```

**Note**: The Spotfire Server chart requires some variables to start (such as database server connection details).
See examples and variables description below.

See [helm install](https://helm.sh/docs/helm/helm_install/) for command documentation.

#### Installing with a new database

This example shows how to deploy a Spotfire Server and a PostgreSQL database in a kubernetes cluster using helm charts. 
The database is deployed as a container that, by default, creates its own PVC for storage. 
You can use any of the supported databases as a container, a VM, a bare metal server, or as cloud database service.

More information related to supported databases and their configuration can be found [here](#supported-databases-configuration)

Steps:

1. Add the bitnami charts repository and install a postgresql database using [Bitnami's PostgreSQL chart](https://artifacthub.io/packages/helm/bitnami/postgresql):
    ```bash
    helm repo add bitnami https://charts.bitnami.com/bitnami
    helm install vanilla-tssdb bitnami/postgresql
    ```

2. Export the postgresql autogenerated random password:
    ```bash
    export POSTGRES_PASSWORD=$(kubectl get secret --namespace default vanilla-tssdb-postgresql -o jsonpath="{.data.postgres-password}" | base64 --decode)
    ```

3. Install the Spotfire Server chart using the release name `my-release`:
    ```bash
    helm upgrade --install vanilla-tss . \
        --set global.spotfire.image.registry="127.0.0.1:32000" \
        --set global.spotfire.image.pullPolicy="Always" \
        --set database.bootstrap.databaseUrl="jdbc:postgresql://vanilla-tssdb-postgresql.default.svc.cluster.local/" \
        --set database.create-db.databaseUrl="jdbc:postgresql://vanilla-tssdb-postgresql.default.svc.cluster.local/" \
        --set database.create-db.adminUsername=postgres \
        --set database.create-db.adminPassword="$POSTGRES_PASSWORD" \
        --set database.create-db.enabled=true \
        --set site.publicAddress=http://localhost/
    ```

   **Note**: You must provide your private registry address where the Spotfire containers are stored.

   **Note**: This example assumes that your Spotfire container images are located in a configured registry at 127.0.0.1:32000.
   See the Kubernetes documentation for how to [Pull an Image from a Private Registry](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/), 
   or how to configure a local registry in your Kubernetes distribution (for example, [microk8s built-in registry](https://microk8s.io/docs/registry-built-in)).

4. Export the autogenerated Spotfire admin password:
    ```bash
    export SPOTFIREADMIN_PASSWORD=$(kubectl get secrets vanilla-tss-spotfire-server-spotfireadmin -o jsonpath="{.data.SPOTFIREADMIN_PASSWORD}" | base64 --decode)
    ```

5. Export the autogenerated Spotfire database password for user spotfire:
    ```bash
    export SPOTFIREDB_PASSWORD=$(kubectl get secrets vanilla-tss-spotfire-server-database -o jsonpath="{.data.SPOTFIREDB_PASSWORD}" | base64 --decode)
    ```

6. Access the Spotfire Server web interface.

For more information on Spotfire, see [TIBCO Spotfire® Server - Installation and Administration](https://docs.tibco.com/pub/spotfire_server/latest/doc/html/TIB_sfire_server_tsas_admin_help/server/topics/administration.html) documentation

For more information on PostgreSQL deployment using the chart in the example, see [Bitnami's PostgreSQL chart](https://artifacthub.io/packages/helm/bitnami/postgresql) documentation.

#### Installing with an existing database

Deploy Spotfire Server using an existing database
**Note**: Before running the following command, set the proposed variables in your environment, or edit the command, replacing the proposed variables with the corresponding values.

```bash
helm install my-release . \
    --set database.create-db.adminUsername="$DB_ADMIN" \
    --set database.create-db.adminPassword="$DB_PASSWORD" \
    --set database.bootstrap.databaseUrl="$DB_DRIVER_URL" \
    --set database.bootstrap.password="$SPOTFIREDB_PASSWORD"
```

Additional database connection configuration is typically done through the JDBC connection properties in the connection url and varies between different database and driver vendors, e.g [Postgres JDBC](https://jdbc.postgresql.org/documentation/head/connect.html). 

In some specific cases there are also a need to place files in the container (e.g for TLS connections) and supply the absolute path of these files in the connection url, use `extraVolumes`, `extraVolumeMounts`, `cliPod.extraVolumes`, `cliPod.extraVolumeMounts`, `configJob.extraVolumes`, `configJob.extraVolumeMounts` in [Volumes](#volumes) for this.

### Uninstalling

To uninstall the `my-release` deployment:
```bash
helm uninstall my-release
```

See [helm uninstall](https://helm.sh/docs/helm/helm_uninstall/) for command documentation.

#### Deleting any remaining resources

Normally, all services and pods are deleted using `helm uninstall`, but occasionally you might need to manually delete existing Spotfire persistence volume claims 
or jobs that are uncompleted due to interrupted operation or incorrect setup.

- To free up the storage, manually deleting all persistent volume claims:
```bash
kubectl get pvc
```

- To delete only the persistent volume claims that you do not want to keep:
```bash
kubectl delete pvc <PVC ids here>
```

- To delete the release-related jobs:
```bash
kubectl delete job.batch/my-release-spotfire-server-basic-config-job
```

Alternatively, delete the kubernetes namespace that contains these resources.

### Scaling

For scaling the `my-release` deployment, do a helm upgrade, providing the target number of pod instances in the `replicaCount` variable.
```bash
helm upgrade --install my-release . --reuse-values --set replicaCount=3
```

#### Autoscaling with KEDA

To use [KEDA](https://keda.sh/docs) for autoscaling, first install it in the Kubernetes cluster. You must also install a Prometheus instance that scrapes metrics from the Spotfire pods.

Example: A `values.yml` snippet configuration for enabling autoscaling with KEDA:
```
kedaAutoscaling:
  enabled: true
  spotfireConfig:
    prometheusServerAddress: http://prometheus-server.monitor.svc.cluster.local
  threshold: 60
  minReplicas: 1
  maxReplicas: 3
```

The value specified for threshold determines the value that the query must reach before the service is scaled.

The `spotfire-server` has the following defaults:
- The default autoscaling metric is the `spotfire_OS_OperatingSystem_ProcessCpuLoad`.
- The default query is the sum of CPU usage (in percent) of all of the Spotfire Server instances.

With these settings, if the total CPU usage for all instances is greater than the threshold, then another instance is scaled out; 
If the total CPU usage for all instances falls under the threshold, then the instance is scaled in.

For each multiple of the `kedaAutoscaling.threshold`, another instance is scaled out.
- With 1 replica, if the query value is at >60% CPU, another replica is created.
- With 2 replicas, if the query value is at >120%, another replica is created, and so on.

See [HPA algorithm details](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#algorithm-details).

**Note**: To allow time for draining sessions when scaling in, tune the values for `draining.minimumSeconds` and `draining.timeoutSeconds`.

For more advanced scenarios, see [kedaAutoscaling.advanced](https://keda.sh/docs/latest/concepts/scaling-deployments/#advanced) and [kedaAutoscaling.fallback](https://keda.sh/docs/latest/concepts/scaling-deployments/#fallback).

Additionally, you can define your own [custom scaling triggers](https://keda.sh/docs/latest/concepts/scaling-deployments/#triggers). Helm template functionality is available:
```
kedaAutoscaling:
  triggers:
  # {list of triggers to activate scaling of the target resource}
```

### Upgrading

To upgrade the `my-release` deployment:
```bash
helm upgrade --install my-release .
```

See [helm upgrade](https://helm.sh/docs/helm/helm_upgrade/) for command documentation.

#### Upgrading the Spotfire Server version

**Note**: When you upgrade to a newer Spotfire Server version and newer Spotfire services versions, upgrade the Spotfire Server first, and then upgrade the Spotfire services.

By default, when you upgrade or install to a helm chart that deploys a newer version of TIBCO Spotfire Server,
the Spotfire database is upgraded to the schema version required by the newer Spotfire Server version.

- To disable the automatic Spotfire database schema upgrade, set the value `database.upgrade` to `false`. In this case, you must manually upgrade the database.
- To enable rolling back an upgrade, before you upgrade to a newer Spotfire Server version, first back up the Spotfire database. 

If you use an external library storage, create a snapshot corresponding to the database backup.

The Kubernetes job `config-job' upgrades the Spotfire database schema. To make sure the upgrade is successful, check the `config-job' logs.

The job `config-job` uses the Spotfire Server Upgrade Tool.
For details, see [Run the Spotfire Server Upgrade Tool](https://docs.tibco.com/pub/spotfire_server/latest/doc/html/TIB_sfire_server_tsas_admin_help/server/topics/run_the_spotfire_server_upgrade_tool.html).

#### Running custom spotfire-config tool tasks during helm release upgrade / installation

During a helm release upgrade and installation, you can run the custom spotfire-config tool tasks to add a custom configuration or to run custom tasks. 
There are two types of config.sh scripts that you can run.

- `configuration.configurationScripts` - Scripts that modify the Spotfire server configuration (configuration.xml).
- `configuration.commandScripts` - Scripts that do not modify the Spotfire server configuration (for example, for creating users, assigning licenses, and so on).

**Note**: To deploy SDN files during a helm release upgrade or installation, see [Using persistent volumes for deploying SDNs/SPKs](#Using-persistent-volumes-for-deploying-SDNs/SPKs).

Example yaml file to use with `helm upgrade` or `helm install`:
```yaml
configuration:
  configurationScripts:
    - name: my_custom_script
      script: |
        echo "This is an example custom configuration tasks. "
        set-config-prop --name=lifecycle.changes-monitoring.draining.timeout-seconds --value=180 --configuration="${CONFIGURATION_FILE}" --bootstrap-config="${BOOTSTRAP_FILE}"
    - name: my_second_script
      script: |
        echo "This script will be executed after the one above."
        echo "Scripts are executed in the order in which they appear the values file."

  commandScripts:
    - name: my_commands_script
      script: create-user --bootstrap-config="${BOOTSTRAP_FILE}" --tool-password="${TOOL_PASSWORD}" --username="my_new_user" --password="password" --ignore-existing=true
```

You can use environment variables in the scripts. These include, but are not limited to, `CONFIGURATION_FILE`, `BOOTSTRAP_FILE` and `TOOL_PASSWORD`.

For more information about config.sh scripts, see the Spotfire Server documentation about [scripting a configuration](https://docs.tibco.com/pub/spotfire_server/latest/doc/html/TIB_sfire_server_tsas_admin_help/server/topics/scripting_a_configuration.html).

If your scripts require additional environment variables, to add environment variables from existing secrets or configmaps, use `extraEnvVarsSecret` or `extraEnvVarsCM`.

## Volumes

The spotfire-server chart uses volumes to mount external files into the containers file system to make them available to the containers. You can also use volumes to persist data that is written by the application 
to an external volume.

Setting up permission is usually handled by the Kubernetes administrators. See the Kubernetes documentation for best practices.

**Note**: You can create a pod running as the user `root` that uses a PersistentVolume or PersistentVolumeClaim to set the right permissions or to pre-populate the volume with jar files or library import files.

- For more information on volumes, see [Volumes](https://kubernetes.io/docs/concepts/storage/volumes/) and [Persistent Volumes](https://kubernetes.io/docs/concepts/storage/persistent-volumes/).
- For more information using PersistentVolumes and PersistentVolumeClaims in a Pod, see [Configure a Pod to Use a PersistentVolume for Storage](https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/).

### extraVolumes and extraVolumesMounts

A general way use additional volumes is with `extraVolumeMounts` and `extraVolumes`. 

For extra volumes: 
- In the spotfire-server pod, use `extraVolumeMounts` and `extraVolumes`.
- In the cliPod, use `cliPod.extraVolumeMounts` and `cliPod.extraVolumes`. 
- In the configJob, use `configJob.extraVolumeMounts` and `configJob.extraVolumes`.

Example:
```yaml
extraVolumeMounts:
  - name: example
    mountPath: /opt/tibco/example.txt
    subPath: example.txt
extraVolumes:
  - name: example
    persistentVolumeClaim:
      claimName: exampleClaim
```

### Spotfire specific volumes

You can mount a number of Spotfire specific volumes. If a volume is not configured, it is not used, or the mountPath uses an emptyDir volume.

- `volumes.certificates` - Used by spotfire-server, configJob, cliPod
- `volumes.customExt` - Used by configJob, spotfire-server, cliPod
- `volumes.deployments` - Used by configJob
- `volumes.libraryImportExport` - Used by spotfire-server
- `volumes.troubleshooting` - Used by spotfire-server

#### Volume for library exporting and importing

**Note**: The `spotfire` user needs read and write permissions for the volume.

*mountPath*: /opt/tibco/tss/tomcat/custom-ext (spotfire-server) or /opt/tibco/spotfireconfigtool/custom-ext (configJob, cliPod)

You can use Kubernetes persistent volumes as a shared storage location for the _Spotfire library_ import and export location, custom jars, deployment packages, and other features.

When [importing to library](https://docs.tibco.com/pub/sfire-analyst/latest/doc/html/en-US/TIB_sfire-analyst_UsersGuide/index.htm#t=lib%2Flib_importing_to_library.htm&rhsearch=export&rhsyns=%20)
or [exporting from library](https://docs.tibco.com/pub/sfire-analyst/latest/doc/html/en-US/TIB_sfire-analyst_UsersGuide/index.htm#t=lib%2Flib_exporting_from_library.htm&rhsearch=export&rhsyns=%20),
you can use `volumes.libraryImportExport.persistentVolumeClaim` or `volumes.libraryImportExport.persistentVolumeClaim.existingClaim` to control which PersistentVolume or PersistentVolumeClaim to use.

*mountPath*: /opt/tibco/tss/tomcat/application-data/library on spotfire-server

#### Volume for custom jar files

**Note**: The `spotfire` user needs read permissions for the volume.

To use custom jar files in the `spotfire-server` container, you can create a volume with the desired files to mount on container start. Use `volumes.customExt.existingClaim` to set the PersistentVolumeClaim to use.

For information on using additional Java library files for Spotfire Server, see:
 - [Installing database drivers for Information Designer
](https://docs.tibco.com/pub/spotfire_server/latest/doc/html/TIB_sfire_server_tsas_admin_help/server/topics/installing_database_drivers_for_information_designer.html)
 - [Authentication towards a custom JAAS module
](https://docs.tibco.com/pub/spotfire_server/latest/doc/html/TIB_sfire_server_tsas_admin_help/server/topics/authentication_towards_a_custom_jaas_module.html)
 - [Post-authentication filter](https://docs.tibco.com/pub/spotfire_server/latest/doc/html/TIB_sfire_server_tsas_admin_help/server/topics/post-authentication_filter.html)


#### Deployment volume for deploying SDNs/SPKs

*mountPath*: /opt/tibco/spotfireconfigtool/deployments on configJob

To deploy SDNs/SPKs files in _Spotfire deployment area_, you can create a volume with the desired files to mount on container start.

1. Copy the desired SDNs/SPKs in a folder (such as `Test/`) in the volume. 
The helm configuration job creates a _Spotfire deployment area_ with the folder name (if it does not exist), and the packages are deployed in that area. 
2. Set the PersistentVolumeClaim to use with `volumes.deployments.existingClaim`.

Example: The following volume file structure creates the deployment areas "Production" and "Test" and deploys the provided SDN files in these deployment areas:
```txt
Production/Spotfire.Dxp.sdn
Test/Spotfire.Dxp.sdn
Test/Spotfire.Dxp.PythonServiceLinux.sdn
```

**Note**: The Spotfire deployment area names are case insensitive and have a maximum length of 25 characters. These are the valid characters:
* [a-z]
* [0-9]
* The underline character `_`
* The dash character `-`

For more information, see [Spotfire Deployments and deployment areas](https://docs.tibco.com/pub/spotfire_server/latest/doc/html/TIB_sfire_server_tsas_admin_help/server/topics/deployments_and_deployment_areas.html)

#### Certificates volume

**Note**: The `spotfire` user needs read permissions to the volume.

*mountPath*: /opt/tibco/tss/tomcat/certs (spotfire-server) or /opt/tibco/spotfireconfigtool/certs (configJob, cliPod)

If you use self-signed or custom certificates for connecting to LDAPS (such as `.jks` keystore files), you can create a volume with the desired files to mount on container start.

To control which PersistentVolumeClaim to use, use `volumes.certifacates.existingClaim`. 

For more information on using self-signed certificates for LDAPS with the Spotfire Server, see [Configuring LDAPS](https://docs.tibco.com/pub/spotfire_server/latest/doc/html/TIB_sfire_server_tsas_admin_help/server/topics/configuring_ldaps.html).


#### Troubleshooting volume

*mountPath*: /opt/tibco/troubleshooting/jvm-heap-dumps on spotfire-server

spotfire-server uses the volume to write troubleshooting information such as heap dumps to this path. To persist this information, set `volumes.troubleshooting.existingClaim`.

## Always-on Spotfire configuration CLI pod

When `cliPod.enabled` is set to `true`, an always-on Spotfire configuration CLI pod is deployed with the chart. You can use this pod to manage and configure the Spotfire environment.

Example: Get the bash prompt into the configuration CLI pod:
```bash
$ kubectl exec -it $(kubectl get pods -l "app.kubernetes.io/component=cli, app.kubernetes.io/part-of=spotfire" -o jsonpath="{.items[0].metadata.name}" ) -- bash
my-release-cli-859fdc8cdf-d58rf $
```

For more information, see [Configuration using the command line](https://docs.tibco.com/pub/spotfire_server/latest/doc/html/TIB_sfire_server_tsas_admin_help/server/topics/configuration_using_the_command_line.html).

## Additional / custom environment variables

You can use the following value keys to add additional environment variables to the pods.

- `extraEnvVars`, `extraEnvVarsCM`, `extraEnvVarsSecret` - Extra environment variables for Spotfire Server
- `cliPod.extraEnvVars`, `cliPod.extraEnvVarsCM`, `cliPod.extraEnvVarsSecret` - Extra environment variables for cliPod
- `configJob.extraEnvVars`, `configJob.extraEnvVarsCM`, `configJob.extraEnvVarsSecret` - Extra environment variables for configJob

Use these to set options such as having custom initialization and configuration scripts consume variables, or setting options for the Spotfire Server JVM and Tomcat.

Use `extraEnvVarsSecret` or `extraEnvVarsCM` to add environment variables from existing secrets or configmaps.

Example: Set JVM settings for Spotfire Server release:
```bash
helm install my-release -f my-values.yml .
```

my-values.yaml:
```yaml
extraEnvVars:
  - name: CATALINA_INITIAL_HEAPSIZE
    value: 1024m
  - name: CATALINA_MAXIMUM_HEAPSIZE
    value: 2048m
  - name: CATALINA_OPTS
    value: "-Djava.net.preferIPv4Stack=true"
```
## Supported databases configuration

| Database | driver class | create-db.databaseUrl | bootstrap.databaseUrl | Additional parameters |
|----------|--------------|-----------------------|-----------------------|-----------------------|
| Postgres | org.postgresql.Driver | jdbc:postgresql://databasehost:databaseport/ | jdbc:postgresql://databasehost:databaseport/ |  | 
| Oracle | oracle.jdbc.OracleDriver | jdbc:oracle:thin:@//databasehost:databaseport/service | jdbc:oracle:thin:@//databasehost:databaseport/service | oracleRootfolder, oracleTablespacePrefix |
| MSSQL | com.microsoft.sqlserver.jdbc.SQLServerDriver | jdbc:sqlserver://databasehost:databaseport | jdbc:sqlserver://databasehost:databaseport;DatabaseName=databasename | |
| AWS Postgres | org.postgresql.Driver | jdbc:postgresql://databasehost:databaseport/databasename | jdbc:postgresql://databasehost:databaseport/databasename | doNotCreateUser = true | 
| Aurora Postgres | org.postgresql.Driver | jdbc:postgresql://databasehost:databaseport/databasename | jdbc:postgresql://databasehost:databaseport/databasename | doNotCreateUser = true | 
| AWS Oracle | oracle.jdbc.OracleDriver | jdbc:oracle:thin:@databasehost:databaseport/ORCL | jdbc:oracle:thin:@databasehost:databaseport/ORCL | variant = rds |
| AWS MSSQL | com.microsoft.sqlserver.jdbc.SQLServerDriver | jdbc:sqlserver://databasehost:databaseport | jdbc:sqlserver://databasehost:databaseport;DatabaseName=databaseName | variant = rds |
| Azure Postgres | org.postgresql.Driver | jdbc:postgresql://databasehost:databaseport/databasename | jdbc:postgresql://databasehost:databaseport/databasename | doNotCreateUser = true | 
| Azure MSSQL | com.microsoft.sqlserver.jdbc.SQLServerDriver | jdbc:sqlserver://databasehost:databaseport | jdbc:sqlserver://databasehost:databaseport;DatabaseName=databaseName | variant = azure |
| Google Cloud Postgres | org.postgresql.Driver | jdbc:postgresql://databasehost:databaseport/ | jdbc:postgresql://databasehost:databaseport/databasename | doNotCreateUser = true | 



{{ template "chart.valuesSection" . }}

