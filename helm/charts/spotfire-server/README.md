# spotfire-server

![Version: 0.1.2](https://img.shields.io/badge/Version-0.1.2-informational?style=flat-square) ![Type: application](https://img.shields.io/badge/Type-application-informational?style=flat-square) ![AppVersion: 12.0.0](https://img.shields.io/badge/AppVersion-12.0.0-informational?style=flat-square)

A Helm chart for TIBCO Spotfire Server.

**Homepage:** <https://github.com/TIBCO/Spotfire-cloud-deployment-kit>

## Source Code

* <https://github.com/TIBCO/Spotfire-cloud-deployment-kit>

## Requirements

| Repository | Name | Version |
|------------|------|---------|
| file://../spotfire-common | spotfire-common | 0.1.2 |
| https://fluent.github.io/helm-charts | log-forwarder(fluent-bit) | 0.20.5 |
| https://haproxytech.github.io/helm-charts | haproxy | 1.15.0 |

## Overview

This chart deploys the [TIBCO Spotfire® Server](https://docs.tibco.com/pub/spotfire_server/latest/doc/html/TIB_sfire_server_tsas_admin_help/server/topics/introduction_to_the_tibco_spotfire_environment.html) on a [Kubernetes](http://kubernetes.io/) cluster using the [Helm](https://helm.sh/) package manager.

Using this chart, you can also deploy the following:
- The required Spotfire Server database schemas on a supported database server (for example, Postgres).
- A reverse proxy ([HAProxy](https://www.haproxy.org/)) for accessing the Spotfire Server cluster service, with session affinity for external HTTP access.
- An ([Ingress](https://kubernetes.io/docs/concepts/services-networking/ingress/)) with routing rules for accessing the configured reverse proxy.
- Shared storage locations ([Persistent Volumes](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)) for the Spotfire library import and export, custom jars, deployment packages, and other features.

The Spotfire Server pod includes:
- A [Fluent Bit](https://fluentbit.io/) sidecar container for log forwarding.
- Service annotations for [Prometheus](https://prometheus.io/) scrapers. The Prometheus server discovers the service endpoint using these specifications and scrapes metrics from the exporter.

This chart is tested to work with [NGINX Ingress Controller](https://github.com/kubernetes/ingress-nginx), [Elasticsearch](https://www.elastic.co/elasticsearch/) and [Prometheus](https://prometheus.io/).

## Prerequisites

- Familiarity with kubernetes, containers and helm concepts and usage
- Helm 3+
- Ingress controller (optional)
- PV (Persistent Volume) provisioner support in the underlying infrastructure (optional)
- A supported database server for use as the Spotfire database (For information on supported databases, see [Spotfire Server requirements](https://docs.tibco.com/pub/spotfire/general/sr/sr/topics/system_requirements_for_spotfire_products.html).)

**Note**: You can install the database server in the same kubernetes cluster or on premises. Alternatively, you can use a cloud database service.

## Usage

### Installing

To install the chart with the release name `my-release` and the values from the file `my-values`:
```bash
helm install my-release -f my-values.yml .
```

**Note**: The Spotfire Server chart requires some variables to start (such as database server connection details).
See examples and variables description below.

See [helm install](https://helm.sh/docs/helm/helm_install/) for command documentation.

#### Installing with a new database

This example shows how to deploy a Spotfire Server and a PostgreSQL database in a kubernetes cluster using helm charts.
The database is deployed as a container that, by default, creates its own PVC for storage.
You can use any of the supported databases as a container, a VM, a bare metal server, or as cloud database service.

More information related to supported databases and their configuration can be found [here](#supported-databases-configuration)

Steps:

1. Add the bitnami charts repository and install a postgresql database using [Bitnami's PostgreSQL chart](https://artifacthub.io/packages/helm/bitnami/postgresql):
    ```bash
    helm repo add bitnami https://charts.bitnami.com/bitnami
    helm install vanilla-tssdb bitnami/postgresql
    ```

2. Export the postgresql autogenerated random password:
    ```bash
    export POSTGRES_PASSWORD=$(kubectl get secret --namespace default vanilla-tssdb-postgresql -o jsonpath="{.data.postgres-password}" | base64 --decode)
    ```

3. Install the Spotfire Server chart using the release name `my-release`:
    ```bash
    helm upgrade --install vanilla-tss . \
        --set global.spotfire.image.registry="127.0.0.1:32000" \
        --set global.spotfire.image.pullPolicy="Always" \
        --set database.bootstrap.databaseUrl="jdbc:postgresql://vanilla-tssdb-postgresql.default.svc.cluster.local/" \
        --set database.create-db.databaseUrl="jdbc:postgresql://vanilla-tssdb-postgresql.default.svc.cluster.local/" \
        --set database.create-db.adminUsername=postgres \
        --set database.create-db.adminPassword="$POSTGRES_PASSWORD" \
        --set database.create-db.enabled=true \
        --set site.publicAddress=http://localhost/
    ```

   **Note**: You must provide your private registry address where the Spotfire containers are stored.

   **Note**: This example assumes that your Spotfire container images are located in a configured registry at 127.0.0.1:32000.
   See the Kubernetes documentation for how to [Pull an Image from a Private Registry](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/),
   or how to configure a local registry in your Kubernetes distribution (for example, [microk8s built-in registry](https://microk8s.io/docs/registry-built-in)).

4. Export the autogenerated Spotfire admin password:
    ```bash
    export SPOTFIREADMIN_PASSWORD=$(kubectl get secrets vanilla-tss-spotfire-server-spotfireadmin -o jsonpath="{.data.SPOTFIREADMIN_PASSWORD}" | base64 --decode)
    ```

5. Export the autogenerated Spotfire database password for user spotfire:
    ```bash
    export SPOTFIREDB_PASSWORD=$(kubectl get secrets vanilla-tss-spotfire-server-database -o jsonpath="{.data.SPOTFIREDB_PASSWORD}" | base64 --decode)
    ```

6. Access the Spotfire Server web interface.

For more information on Spotfire, see [TIBCO Spotfire® Server - Installation and Administration](https://docs.tibco.com/pub/spotfire_server/latest/doc/html/TIB_sfire_server_tsas_admin_help/server/topics/administration.html) documentation

For more information on PostgreSQL deployment using the chart in the example, see [Bitnami's PostgreSQL chart](https://artifacthub.io/packages/helm/bitnami/postgresql) documentation.

#### Installing with an existing database

Deploy Spotfire Server using an existing database
**Note**: Before running the following command, set the proposed variables in your environment, or edit the command, replacing the proposed variables with the corresponding values.

```bash
helm install my-release . \
    --set database.create-db.adminUsername="$DB_ADMIN" \
    --set database.create-db.adminPassword="$DB_PASSWORD" \
    --set database.bootstrap.databaseUrl="$DB_DRIVER_URL" \
    --set database.bootstrap.password="$SPOTFIREDB_PASSWORD"
```

Additional database connection configuration is typically done through the JDBC connection properties in the connection url and varies between different database and driver vendors, e.g [Postgres JDBC](https://jdbc.postgresql.org/documentation/head/connect.html).

In some specific cases there are also a need to place files in the container (e.g for TLS connections) and supply the absolute path of these files in the connection url, use `extraVolumes`, `extraVolumeMounts`, `cliPod.extraVolumes`, `cliPod.extraVolumeMounts`, `configJob.extraVolumes`, `configJob.extraVolumeMounts` in [Volumes](#volumes) for this.

### Uninstalling

To uninstall the `my-release` deployment:
```bash
helm uninstall my-release
```

See [helm uninstall](https://helm.sh/docs/helm/helm_uninstall/) for command documentation.

#### Deleting any remaining resources

Normally, all services and pods are deleted using `helm uninstall`, but occasionally you might need to manually delete existing Spotfire persistence volume claims
or jobs that are uncompleted due to interrupted operation or incorrect setup.

- To free up the storage, manually deleting all persistent volume claims:
```bash
kubectl get pvc
```

- To delete only the persistent volume claims that you do not want to keep:
```bash
kubectl delete pvc <PVC ids here>
```

- To delete the release-related jobs:
```bash
kubectl delete job.batch/my-release-spotfire-server-basic-config-job
```

Alternatively, delete the kubernetes namespace that contains these resources.

### Scaling

For scaling the `my-release` deployment, do a helm upgrade, providing the target number of pod instances in the `replicaCount` variable.
```bash
helm upgrade --install my-release . --reuse-values --set replicaCount=3
```

#### Autoscaling with KEDA

To use [KEDA](https://keda.sh/docs) for autoscaling, first install it in the Kubernetes cluster. You must also install a Prometheus instance that scrapes metrics from the Spotfire pods.

Example: A `values.yml` snippet configuration for enabling autoscaling with KEDA:
```
kedaAutoscaling:
  enabled: true
  spotfireConfig:
    prometheusServerAddress: http://prometheus-server.monitor.svc.cluster.local
  threshold: 60
  minReplicas: 1
  maxReplicas: 3
```

The value specified for threshold determines the value that the query must reach before the service is scaled.

The `spotfire-server` has the following defaults:
- The default autoscaling metric is the `spotfire_OS_OperatingSystem_ProcessCpuLoad`.
- The default query is the sum of CPU usage (in percent) of all of the Spotfire Server instances.

With these settings, if the total CPU usage for all instances is greater than the threshold, then another instance is scaled out;
If the total CPU usage for all instances falls under the threshold, then the instance is scaled in.

For each multiple of the `kedaAutoscaling.threshold`, another instance is scaled out.
- With 1 replica, if the query value is at >60% CPU, another replica is created.
- With 2 replicas, if the query value is at >120%, another replica is created, and so on.

See [HPA algorithm details](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#algorithm-details).

**Note**: To allow time for draining sessions when scaling in, tune the values for `draining.minimumSeconds` and `draining.timeoutSeconds`.

For more advanced scenarios, see [kedaAutoscaling.advanced](https://keda.sh/docs/latest/concepts/scaling-deployments/#advanced) and [kedaAutoscaling.fallback](https://keda.sh/docs/latest/concepts/scaling-deployments/#fallback).

Additionally, you can define your own [custom scaling triggers](https://keda.sh/docs/latest/concepts/scaling-deployments/#triggers). Helm template functionality is available:
```
kedaAutoscaling:
  triggers:
  # {list of triggers to activate scaling of the target resource}
```

### Upgrading

To upgrade the `my-release` deployment:
```bash
helm upgrade --install my-release .
```

See [helm upgrade](https://helm.sh/docs/helm/helm_upgrade/) for command documentation.

#### Upgrading the Spotfire Server version

**Note**: When you upgrade to a newer Spotfire Server version and newer Spotfire services versions, upgrade the Spotfire Server first, and then upgrade the Spotfire services.

By default, when you upgrade or install to a helm chart that deploys a newer version of TIBCO Spotfire Server,
the Spotfire database is upgraded to the schema version required by the newer Spotfire Server version.

- To disable the automatic Spotfire database schema upgrade, set the value `database.upgrade` to `false`. In this case, you must manually upgrade the database.
- To enable rolling back an upgrade, before you upgrade to a newer Spotfire Server version, first back up the Spotfire database.

If you use an external library storage, create a snapshot corresponding to the database backup.

The Kubernetes job `config-job' upgrades the Spotfire database schema. To make sure the upgrade is successful, check the `config-job' logs.

The job `config-job` uses the Spotfire Server Upgrade Tool.
For details, see [Run the Spotfire Server Upgrade Tool](https://docs.tibco.com/pub/spotfire_server/latest/doc/html/TIB_sfire_server_tsas_admin_help/server/topics/run_the_spotfire_server_upgrade_tool.html).

#### Running custom spotfire-config tool tasks during helm release upgrade / installation

During a helm release upgrade and installation, you can run the custom spotfire-config tool tasks to add a custom configuration or to run custom tasks.
There are two types of config.sh scripts that you can run.

- `configuration.configurationScripts` - Scripts that modify the Spotfire server configuration (configuration.xml).
- `configuration.commandScripts` - Scripts that do not modify the Spotfire server configuration (for example, for creating users, assigning licenses, and so on).

**Note**: To deploy SDN files during a helm release upgrade or installation, see [Using persistent volumes for deploying SDNs/SPKs](#Using-persistent-volumes-for-deploying-SDNs/SPKs).

Example yaml file to use with `helm upgrade` or `helm install`:
```yaml
configuration:
  configurationScripts:
    - name: my_custom_script
      script: |
        echo "This is an example custom configuration tasks. "
        set-config-prop --name=lifecycle.changes-monitoring.draining.timeout-seconds --value=180 --configuration="${CONFIGURATION_FILE}" --bootstrap-config="${BOOTSTRAP_FILE}"
    - name: my_second_script
      script: |
        echo "This script will be executed after the one above."
        echo "Scripts are executed in the order in which they appear the values file."

  commandScripts:
    - name: my_commands_script
      script: create-user --bootstrap-config="${BOOTSTRAP_FILE}" --tool-password="${TOOL_PASSWORD}" --username="my_new_user" --password="password" --ignore-existing=true
```

You can use environment variables in the scripts. These include, but are not limited to, `CONFIGURATION_FILE`, `BOOTSTRAP_FILE` and `TOOL_PASSWORD`.

For more information about config.sh scripts, see the Spotfire Server documentation about [scripting a configuration](https://docs.tibco.com/pub/spotfire_server/latest/doc/html/TIB_sfire_server_tsas_admin_help/server/topics/scripting_a_configuration.html).

If your scripts require additional environment variables, to add environment variables from existing secrets or configmaps, use `extraEnvVarsSecret` or `extraEnvVarsCM`.

## Volumes

The spotfire-server chart uses volumes to mount external files into the containers file system to make them available to the containers. You can also use volumes to persist data that is written by the application
to an external volume.

Setting up permission is usually handled by the Kubernetes administrators. See the Kubernetes documentation for best practices.

**Note**: You can create a pod running as the user `root` that uses a PersistentVolume or PersistentVolumeClaim to set the right permissions or to pre-populate the volume with jar files or library import files.

- For more information on volumes, see [Volumes](https://kubernetes.io/docs/concepts/storage/volumes/) and [Persistent Volumes](https://kubernetes.io/docs/concepts/storage/persistent-volumes/).
- For more information using PersistentVolumes and PersistentVolumeClaims in a Pod, see [Configure a Pod to Use a PersistentVolume for Storage](https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/).

### extraVolumes and extraVolumesMounts

A general way use additional volumes is with `extraVolumeMounts` and `extraVolumes`.

For extra volumes:
- In the spotfire-server pod, use `extraVolumeMounts` and `extraVolumes`.
- In the cliPod, use `cliPod.extraVolumeMounts` and `cliPod.extraVolumes`.
- In the configJob, use `configJob.extraVolumeMounts` and `configJob.extraVolumes`.

Example:
```yaml
extraVolumeMounts:
  - name: example
    mountPath: /opt/tibco/example.txt
    subPath: example.txt
extraVolumes:
  - name: example
    persistentVolumeClaim:
      claimName: exampleClaim
```

### Spotfire specific volumes

You can mount a number of Spotfire specific volumes. If a volume is not configured, it is not used, or the mountPath uses an emptyDir volume.

- `volumes.certificates` - Used by spotfire-server, configJob, cliPod
- `volumes.customExt` - Used by configJob, spotfire-server, cliPod
- `volumes.deployments` - Used by configJob
- `volumes.libraryImportExport` - Used by spotfire-server
- `volumes.troubleshooting` - Used by spotfire-server

#### Volume for library exporting and importing

**Note**: The `spotfire` user needs read and write permissions for the volume.

*mountPath*: /opt/tibco/tss/tomcat/custom-ext (spotfire-server) or /opt/tibco/spotfireconfigtool/custom-ext (configJob, cliPod)

You can use Kubernetes persistent volumes as a shared storage location for the _Spotfire library_ import and export location, custom jars, deployment packages, and other features.

When [importing to library](https://docs.tibco.com/pub/sfire-analyst/latest/doc/html/en-US/TIB_sfire-analyst_UsersGuide/index.htm#t=lib%2Flib_importing_to_library.htm&rhsearch=export&rhsyns=%20)
or [exporting from library](https://docs.tibco.com/pub/sfire-analyst/latest/doc/html/en-US/TIB_sfire-analyst_UsersGuide/index.htm#t=lib%2Flib_exporting_from_library.htm&rhsearch=export&rhsyns=%20),
you can use `volumes.libraryImportExport.persistentVolumeClaim` or `volumes.libraryImportExport.persistentVolumeClaim.existingClaim` to control which PersistentVolume or PersistentVolumeClaim to use.

*mountPath*: /opt/tibco/tss/tomcat/application-data/library on spotfire-server

#### Volume for custom jar files

**Note**: The `spotfire` user needs read permissions for the volume.

To use custom jar files in the `spotfire-server` container, you can create a volume with the desired files to mount on container start. Use `volumes.customExt.existingClaim` to set the PersistentVolumeClaim to use.

For information on using additional Java library files for Spotfire Server, see:
 - [Installing database drivers for Information Designer
](https://docs.tibco.com/pub/spotfire_server/latest/doc/html/TIB_sfire_server_tsas_admin_help/server/topics/installing_database_drivers_for_information_designer.html)
 - [Authentication towards a custom JAAS module
](https://docs.tibco.com/pub/spotfire_server/latest/doc/html/TIB_sfire_server_tsas_admin_help/server/topics/authentication_towards_a_custom_jaas_module.html)
 - [Post-authentication filter](https://docs.tibco.com/pub/spotfire_server/latest/doc/html/TIB_sfire_server_tsas_admin_help/server/topics/post-authentication_filter.html)

#### Deployment volume for deploying SDNs/SPKs

*mountPath*: /opt/tibco/spotfireconfigtool/deployments on configJob

To deploy SDNs/SPKs files in _Spotfire deployment area_, you can create a volume with the desired files to mount on container start.

1. Copy the desired SDNs/SPKs in a folder (such as `Test/`) in the volume.
The helm configuration job creates a _Spotfire deployment area_ with the folder name (if it does not exist), and the packages are deployed in that area.
2. Set the PersistentVolumeClaim to use with `volumes.deployments.existingClaim`.

Example: The following volume file structure creates the deployment areas "Production" and "Test" and deploys the provided SDN files in these deployment areas:
```txt
Production/Spotfire.Dxp.sdn
Test/Spotfire.Dxp.sdn
Test/Spotfire.Dxp.PythonServiceLinux.sdn
```

**Note**: The Spotfire deployment area names are case insensitive and have a maximum length of 25 characters. These are the valid characters:
* [a-z]
* [0-9]
* The underline character `_`
* The dash character `-`

For more information, see [Spotfire Deployments and deployment areas](https://docs.tibco.com/pub/spotfire_server/latest/doc/html/TIB_sfire_server_tsas_admin_help/server/topics/deployments_and_deployment_areas.html)

#### Certificates volume

**Note**: The `spotfire` user needs read permissions to the volume.

*mountPath*: /opt/tibco/tss/tomcat/certs (spotfire-server) or /opt/tibco/spotfireconfigtool/certs (configJob, cliPod)

If you use self-signed or custom certificates for connecting to LDAPS (such as `.jks` keystore files), you can create a volume with the desired files to mount on container start.

To control which PersistentVolumeClaim to use, use `volumes.certifacates.existingClaim`.

For more information on using self-signed certificates for LDAPS with the Spotfire Server, see [Configuring LDAPS](https://docs.tibco.com/pub/spotfire_server/latest/doc/html/TIB_sfire_server_tsas_admin_help/server/topics/configuring_ldaps.html).

#### Troubleshooting volume

*mountPath*: /opt/tibco/troubleshooting/jvm-heap-dumps on spotfire-server

spotfire-server uses the volume to write troubleshooting information such as heap dumps to this path. To persist this information, set `volumes.troubleshooting.existingClaim`.

## Always-on Spotfire configuration CLI pod

When `cliPod.enabled` is set to `true`, an always-on Spotfire configuration CLI pod is deployed with the chart. You can use this pod to manage and configure the Spotfire environment.

Example: Get the bash prompt into the configuration CLI pod:
```bash
$ kubectl exec -it $(kubectl get pods -l "app.kubernetes.io/component=cli, app.kubernetes.io/part-of=spotfire" -o jsonpath="{.items[0].metadata.name}" ) -- bash
my-release-cli-859fdc8cdf-d58rf $
```

For more information, see [Configuration using the command line](https://docs.tibco.com/pub/spotfire_server/latest/doc/html/TIB_sfire_server_tsas_admin_help/server/topics/configuration_using_the_command_line.html).

## Additional / custom environment variables

You can use the following value keys to add additional environment variables to the pods.

- `extraEnvVars`, `extraEnvVarsCM`, `extraEnvVarsSecret` - Extra environment variables for Spotfire Server
- `cliPod.extraEnvVars`, `cliPod.extraEnvVarsCM`, `cliPod.extraEnvVarsSecret` - Extra environment variables for cliPod
- `configJob.extraEnvVars`, `configJob.extraEnvVarsCM`, `configJob.extraEnvVarsSecret` - Extra environment variables for configJob

Use these to set options such as having custom initialization and configuration scripts consume variables, or setting options for the Spotfire Server JVM and Tomcat.

Use `extraEnvVarsSecret` or `extraEnvVarsCM` to add environment variables from existing secrets or configmaps.

Example: Set JVM settings for Spotfire Server release:
```bash
helm install my-release -f my-values.yml .
```

my-values.yaml:
```yaml
extraEnvVars:
  - name: CATALINA_INITIAL_HEAPSIZE
    value: 1024m
  - name: CATALINA_MAXIMUM_HEAPSIZE
    value: 2048m
  - name: CATALINA_OPTS
    value: "-Djava.net.preferIPv4Stack=true"
```
## Supported databases configuration

| Database | driver class | create-db.databaseUrl | bootstrap.databaseUrl | Additional parameters |
|----------|--------------|-----------------------|-----------------------|-----------------------|
| Postgres | org.postgresql.Driver | jdbc:postgresql://databasehost:databaseport/ | jdbc:postgresql://databasehost:databaseport/ |  |
| Oracle | oracle.jdbc.OracleDriver | jdbc:oracle:thin:@//databasehost:databaseport/service | jdbc:oracle:thin:@//databasehost:databaseport/service | oracleRootfolder, oracleTablespacePrefix |
| MSSQL | com.microsoft.sqlserver.jdbc.SQLServerDriver | jdbc:sqlserver://databasehost:databaseport | jdbc:sqlserver://databasehost:databaseport;DatabaseName=databasename | |
| AWS Postgres | org.postgresql.Driver | jdbc:postgresql://databasehost:databaseport/databasename | jdbc:postgresql://databasehost:databaseport/databasename | doNotCreateUser = true |
| Aurora Postgres | org.postgresql.Driver | jdbc:postgresql://databasehost:databaseport/databasename | jdbc:postgresql://databasehost:databaseport/databasename | doNotCreateUser = true |
| AWS Oracle | oracle.jdbc.OracleDriver | jdbc:oracle:thin:@databasehost:databaseport/ORCL | jdbc:oracle:thin:@databasehost:databaseport/ORCL | variant = rds |
| AWS MSSQL | com.microsoft.sqlserver.jdbc.SQLServerDriver | jdbc:sqlserver://databasehost:databaseport | jdbc:sqlserver://databasehost:databaseport;DatabaseName=databaseName | variant = rds |
| Azure Postgres | org.postgresql.Driver | jdbc:postgresql://databasehost:databaseport/databasename | jdbc:postgresql://databasehost:databaseport/databasename | doNotCreateUser = true |
| Azure MSSQL | com.microsoft.sqlserver.jdbc.SQLServerDriver | jdbc:sqlserver://databasehost:databaseport | jdbc:sqlserver://databasehost:databaseport;DatabaseName=databaseName | variant = azure |
| Google Cloud Postgres | org.postgresql.Driver | jdbc:postgresql://databasehost:databaseport/ | jdbc:postgresql://databasehost:databaseport/databasename | doNotCreateUser = true |

## Values

| Key | Type | Default | Description |
|-----|------|---------|-------------|
| affinity | object | `{}` |  |
| cliPod.enabled | bool | `true` |  |
| cliPod.extraEnvVars | list | `[]` | Additional environment variables all spotfire-server pods use.  - name: NAME    value: value |
| cliPod.extraEnvVarsCM | string | `""` |  |
| cliPod.extraEnvVarsSecret | string | `""` |  |
| cliPod.extraInitContainers | list | `[]` | Additional init containers to add to cli pod. More info: `kubectl explain deployment.spec.template.spec.initContainers` |
| cliPod.extraVolumeMounts | list | `[]` | Extra volumeMounts for the configJob Job. More info: `kubectl explain deployment.spec.template.spec.containers.volumeMounts` |
| cliPod.extraVolumes | list | `[]` | Extra volumes for the configJob Job. More info: `kubectl explain deployment.spec.template.spec.volumes` |
| cliPod.image.pullPolicy | string | `nil` | The spotfireConfig image pull policy. Overrides global.spotfire.image.pullPolicy. |
| cliPod.image.pullSecrets | list | `[]` |  |
| cliPod.image.registry | string | `nil` | The image registry for spotfireConfig. Overrides global.spotfire.image.registry value. |
| cliPod.image.repository | string | `"tibco/spotfire-config"` | The spotfireConfig image repository. |
| cliPod.image.tag | string | `"12.0.0-1.0.0"` | The spotfireConfig container image tag to use. |
| cliPod.logLevel | string | `""` | Set to DEBUG or TRACE to increase log level. Defaults to INFO if unset. |
| cliPod.podAnnotations | object | `{}` | Podannotations for cliPod |
| cliPod.podSecurityContext | object | `{}` | The podSecurityContext setting for cliPod More info: `kubectl explain deployment.spec.template.spec.securityContext` |
| cliPod.securityContext | object | `{}` | The securityContext setting for cliPod. More info: `kubectl explain deployment.spec.template.spec.containers.securityContext` |
| configJob.extraEnvVars | list | `[]` | Additional environment variables for all spotfire-server pods to use.  - name: NAME    value: value |
| configJob.extraEnvVarsCM | string | `""` |  |
| configJob.extraEnvVarsSecret | string | `""` |  |
| configJob.extraInitContainers | list | `[]` | Additional init containers to add to the Spotfire server configuration pod. More info: `kubectl explain job.spec.template.spec.initContainers` |
| configJob.extraVolumeMounts | list | `[]` | Extra volumeMounts for the configJob Job. More info: `kubectl explain job.spec.template.spec.containers.volumeMounts` |
| configJob.extraVolumes | list | `[]` | Extra volumes for the configJob Job. More info: `kubectl explain job.spec.template.spec.volumes` |
| configJob.image.pullPolicy | string | `nil` | The spotfireConfig image pull policy. Overrides global.spotfire.image.pullPolicy. |
| configJob.image.pullSecrets | list | `[]` |  |
| configJob.image.registry | string | `nil` | The image registry for spotfireConfig. Overrides global.spotfire.image.registry value. |
| configJob.image.repository | string | `"tibco/spotfire-config"` | The spotfireConfig image repository. |
| configJob.image.tag | string | `"12.0.0-1.0.0"` | The spotfireConfig container image tag to use. |
| configJob.logLevel | string | `""` | Set to DEBUG or TRACE to increase log level. Defaults to INFO if unset. |
| configJob.podAnnotations | object | `{}` | Podannotations for configJob |
| configJob.podSecurityContext | object | `{}` | The podSecurityContext setting for configJob. More info: `kubectl explain job.spec.template.spec.securityContext` |
| configJob.securityContext | object | `{}` | The securityContext setting for configJob. More info: `kubectl explain job.spec.template.spec.containers.securityContext` |
| configJob.ttlSecondsAfterFinished | int | `3600` | Set the length of time in seconds to keep job and its logs until the job is removed. |
| configuration.applyKubernetesConfiguration | bool | `true` | Applies various Spotfire application settings recommended for Kubernetes environments. |
| configuration.commandScripts | list | `[]` | A list of command scripts to run during helm installation or upgrade. Each list item should have the keys 'name' and 'script'. See [config.sh run script](https://docs.tibco.com/pub/spotfire_server/latest/doc/html/TIB_sfire_server_tsas_admin_help/server/topics/scripting_a_configuration.html). Commands in these scripts should NOT operate on configuration.xml. Operations such as adding/removing users and assigning licenses are such typical administrative commands. |
| configuration.configurationScripts | list | `[]` | A list of configuration scripts to apply during helm installation or upgrade. Each list item should have the keys 'name' and 'script'. See [config.sh run script](https://docs.tibco.com/pub/spotfire_server/latest/doc/html/TIB_sfire_server_tsas_admin_help/server/topics/scripting_a_configuration.html). Commands in these scripts should operate only on a local configuration.xml. configuration.xml is automatically imported after all configuration steps run in the order in which they are defined below. |
| configuration.useExisting | bool | `true` | Export the existing Spotfire configuration before applying any additional configuration. If false, then a default configuration is created with `config.sh create-default-config`. |
| database.bootstrap | object | `{"databaseUrl":"jdbc:postgresql://HOSTNAME/","driverClass":"org.postgresql.Driver","password":"","username":"spotfire"}` | For details related to bootstrap properties, visit the product documentation [here](https://docs.tibco.com/pub/spotfire_server/latest/doc/html/TIB_sfire_server_tsas_admin_help/server/topics/bootstrap.html) |
| database.bootstrap.databaseUrl | string | `"jdbc:postgresql://HOSTNAME/"` | The JDBC URL of the database to be used by Spotfire Server |
| database.bootstrap.password | string | `""` | Password to be created for the Spotfire Server database If not provided, this password is automatically generated. |
| database.bootstrap.username | string | `"spotfire"` | Username to be created for the Spotfire Server database. If unset, default value spotfire would be used. |
| database.create-db | object | `{"adminPassword":"","adminUsername":"postgres","databaseUrl":"","doNotCreateUser":false,"enabled":true,"oracleRootfolder":"","oracleTablespacePrefix":"","spotfiredbDbname":"spotfire","variant":""}` | For details related to create-db cli properties, visit the product documentation [here](https://docs.tibco.com/emp/spotfire_server/12.0.0/doc/html/TIB_sfire_server_tsas_admin_help/server/topics/create-db.html) create-db cli also uses properties from database.bootstrap section. |
| database.create-db.adminPassword | string | `""` | Admin password for the database server to be used as the Spotfire Server database |
| database.create-db.adminUsername | string | `"postgres"` | Admin username for the database server to be used as the Spotfire Server database |
| database.create-db.databaseUrl | string | `""` | Like database.bootstrap.databaseUrl but is used for the connection when creating the spotfire database. |
| database.create-db.doNotCreateUser | bool | `false` | Set this to true, in case supported databases(AWS Postgre, Aurora Postgres, Azure Postgres, Google Cloud Postgres) does not allow user creation |
| database.create-db.enabled | bool | `true` | if set to true, Spotfire server schema will also get deployed with other installation |
| database.create-db.oracleRootfolder | string | `""` | Specify the value in case of Oracle database, otherwise keep it blank |
| database.create-db.oracleTablespacePrefix | string | `""` | Specify the value in case of Oracle database, otherwise keep it blank |
| database.create-db.spotfiredbDbname | string | `"spotfire"` | Database name to be created to hold the Spotfire Server database schemas |
| database.create-db.variant | string | `""` | For connecting to MS SQL or Oracle on Amazon RDS, specify 'rds', for MS SQL on Azure, specify 'azure', otherwise omit the option. |
| database.upgrade | bool | `true` | Often new Spotfire server version requires an upgraded database. If true, the database will be upgrade to match the server version being deployed. |
| draining | object | `{"enabled":true,"minimumSeconds":30,"timeoutSeconds":60}` | Configuration of the Spotfire Server container lifecycle PreStop hook. |
| draining.enabled | bool | `true` | Enables or disables the container lifecycle PreStop hook. |
| draining.minimumSeconds | int | `30` | The minimum time in seconds that the server should be draining, even if it is considered idle. |
| draining.timeoutSeconds | int | `60` | The draining timeout in seconds after which the service is forcibly shut down. |
| encryptionPassword | string | `""` | The password for encrypting passwords that are stored in the database. If you do not set this option, then a static password is used. See \-\-encryption-password for the [bootstrap](https://docs.tibco.com/pub/spotfire_server/latest/doc/html/TIB_sfire_server_tsas_admin_help/server/topics/bootstrap.html) command. |
| extraEnvVars | list | `[]` | Additional environment variables that all spotfire-server pods use. |
| extraEnvVarsCM | string | `""` |  |
| extraEnvVarsSecret | string | `""` |  |
| extraInitContainers | list | `[]` | Additional init containers to add to Spotfire server pod. More info: `kubectl explain deployment.spec.template.spec.initContainers` |
| extraVolumeMounts | list | `[]` | Extra volumeMounts for the spotfire-server container. More info: `kubectl explain deployment.spec.template.spec.containers.volumeMounts` |
| extraVolumes | list | `[]` | Extra volumes for the spotfire-server container. More info: `kubectl explain deployment.spec.template.spec.volumes` |
| fluentBitSidecar.image.pullPolicy | string | `"IfNotPresent"` | The image pull policy for the fluent-bit logging sidecar image. |
| fluentBitSidecar.image.repository | string | `"fluent/fluent-bit"` | The image repository for fluent-bit logging sidecar. |
| fluentBitSidecar.image.tag | string | `"1.9.7"` | The image tag to use for fluent-bit logging sidecar. |
| fluentBitSidecar.securityContext | object | `{}` | The securityContext setting for fluent-bit sidecar container. Overrides any securityContext setting on the Pod level. More info: `kubectl explain pod.spec.securityContext` |
| global.spotfire.image.pullPolicy | string | `"IfNotPresent"` | The global container image pull policy. |
| global.spotfire.image.pullSecrets | list | `[]` | The global container image pull secrets. |
| global.spotfire.image.registry | string | `nil` | The global container image registry. Used for TIBCO/Spotfire container images unless overridden. |
| haproxy.config | string | The chart creates a configuration automatically. | The haproxy configuration file template. |
| haproxy.enabled | bool | `true` |  |
| haproxy.extraVolumeMounts[0].mountPath | string | `"/tmp/chart/fix"` |  |
| haproxy.extraVolumeMounts[0].name | string | `"chart-fix"` |  |
| haproxy.extraVolumes[0].emptyDir | object | `{}` |  |
| haproxy.extraVolumes[0].name | string | `"chart-fix"` |  |
| haproxy.kind | string | `"Deployment"` |  |
| haproxy.podAnnotations | object | `{"prometheus.io/path":"/metrics","prometheus.io/port":"1024","prometheus.io/scrape":"true"}` | Prometheus annotations. Should match the haproxy.config settings. |
| haproxy.podLabels."app.kubernetes.io/component" | string | `"haproxy"` |  |
| haproxy.podLabels."app.kubernetes.io/part-of" | string | `"spotfire"` |  |
| haproxy.podSecurityPolicy.create | bool | `false` |  |
| haproxy.service.type | string | `"ClusterIP"` | Sets the service haproxy service proxies traffic to the spotfire-server service. ClusterIP or LoadBalancer. |
| haproxy.spotfireConfig | object | Caching of static resource and debug response headers enabled. | Spotfire specific configuration related to haproxy. |
| haproxy.spotfireConfig.agent.port | int | `9081` | Spotfire Server haproxy agent-port. |
| haproxy.spotfireConfig.cache | object | enabled | Caching of static resources |
| haproxy.spotfireConfig.cleanup.sameSiteCookieAttributeForHttp | bool | `true` | If the SameSite cookie attribute should be removed for HTTP connections in Set-Cookie response headers, then it might be needed in cases where both HTTP and HTTPS are enabled, and upstream servers set this unconditionally. |
| haproxy.spotfireConfig.cleanup.secureCookieAttributeForHttp | bool | `true` | If incorrect, then the secure cookie attribute should be removed for HTTP connections in the Set-Cookie response headers. |
| haproxy.spotfireConfig.debug | bool | `false` | Specifies if debug response headers should be enabled. |
| haproxy.spotfireConfig.loadBalancingCookie | object | stateless load balancing | Cookie-related configuration. |
| haproxy.spotfireConfig.loadBalancingCookie.attributes | string | `"prefix dynamic"` | Attributes for the cookie value in the haproxy config. See https://cbonte.github.io/haproxy-dconv/2.4/configuration.html#4.2-cookie for more information. |
| haproxy.spotfireConfig.loadBalancingCookie.dynamicCookieKey | string | the cookie key | dynamic-cookie-key value in the haproxy config. |
| haproxy.spotfireConfig.timeouts | object | haproxy timeouts | haproxy related timeouts |
| haproxy.spotfireConfig.timeouts.client | string | `"30m"` | https://cbonte.github.io/haproxy-dconv/2.4/configuration.html#4.2-timeout%20client |
| haproxy.spotfireConfig.timeouts.connect | string | `"300ms"` | https://cbonte.github.io/haproxy-dconv/2.4/configuration.html#4.2-timeout%20connect |
| haproxy.spotfireConfig.timeouts.httpRequest | string | `"3600s"` | https://cbonte.github.io/haproxy-dconv/2.4/configuration.html#4.2-timeout%20http-request |
| haproxy.spotfireConfig.timeouts.queue | string | `"60s"` | https://cbonte.github.io/haproxy-dconv/2.4/configuration.html#4-timeout%20queue |
| haproxy.spotfireConfig.timeouts.server | string | `"30m"` | https://cbonte.github.io/haproxy-dconv/2.4/configuration.html#4.2-timeout%20server |
| haproxy.spotfireConfig.timeouts.tunnel | string | `"31m"` | https://cbonte.github.io/haproxy-dconv/2.4/configuration.html#4.2-timeout%20tunnel |
| image.pullPolicy | string | `nil` | The spotfire-server image pull policy. Overrides global.spotfire.image.pullPolicy. |
| image.pullSecrets | list | `[]` | spotfire-server image pull secrets. |
| image.registry | string | `nil` | The image registry for spotfire-server. Overrides global.spotfire.image.registry value. |
| image.repository | string | `"tibco/spotfire-server"` | The spotfire-server image repository. |
| image.tag | string | `"12.0.0-1.0.0"` | The container image tag to use. |
| ingress.enabled | bool | `false` | Enables configuration of ingress to expose Spotfire Server. Requires ingress support in the Kubernetes cluster. |
| ingress.hosts[0].host | string | `"spotfire.local"` |  |
| ingress.hosts[0].paths[0].path | string | `"/"` |  |
| ingress.hosts[0].paths[0].pathType | string | `"Prefix"` |  |
| ingress.tls | list | `[]` |  |
| kedaAutoscaling | object | Disabled | KEDA autoscaling configuration. See https://keda.sh/docs/latest/concepts/scaling-deployment for more details. |
| kedaAutoscaling.cooldownPeriod | int | `300` | The period to wait after the last trigger reported active before scaling the resource back to 0. |
| kedaAutoscaling.maxReplicas | int | `4` | This setting is passed to the HPA definition that KEDA creates for a given resource and holds the maximum number of replicas of the target resource. |
| kedaAutoscaling.minReplicas | int | `1` | The minimum number of replicas KEDA scales the resource down to. |
| kedaAutoscaling.pollingInterval | int | `30` | The interval to check each trigger on. |
| kedaAutoscaling.spotfireConfig | object | `{"prometheusServerAddress":"http://prometheus-server.monitor.svc.cluster.local"}` | Spotfire specific settings |
| kedaAutoscaling.spotfireConfig.prometheusServerAddress | string | `"http://prometheus-server.monitor.svc.cluster.local"` | REQUIRED The URL to the Prometheus server where metrics should be fetched from. |
| livenessProbe.enabled | bool | `true` |  |
| livenessProbe.failureThreshold | int | `3` |  |
| livenessProbe.httpGet.path | string | `"/spotfire/rest/status/getStatus"` |  |
| livenessProbe.httpGet.port | string | `"http"` |  |
| livenessProbe.periodSeconds | int | `3` |  |
| log-forwarder.config.filters | string | Example that drops specific events using [grep](https://docs.fluentbit.io/manual/pipeline/filters/grep) | Add custom fluent-bit [filters configuration](https://docs.fluen tbit.io/manual/pipeline/filters). |
| log-forwarder.config.inputs | string | [tcp input](https://docs.fluentbit.io/manual/pipeline/inputs/tcp) on port 5170 and [forward input](https://docs.fluentbit.io/manual/pipeline/inputs/forward) on port 24224 | fluent-bit [input configuration](https://docs.fluentbit.io/manual/pipeline/inputs). |
| log-forwarder.config.outputs | string | Logs are written to stdout of the log-forwarder pod. | Override this value with an [output configuration](https://docs.fluentbit.io/manual/pipeline/outputs) to send logs to an external system. |
| log-forwarder.enabled | bool | `true` | enables or disables the fluent-bit log-forwarder pod. If enabled, it collects logs from the spotfire-server pods and can forward traffic to any output supported by fluent-bit. |
| log-forwarder.extraPorts[0].containerPort | int | `5170` |  |
| log-forwarder.extraPorts[0].name | string | `"json"` |  |
| log-forwarder.extraPorts[0].port | int | `5170` |  |
| log-forwarder.extraPorts[0].protocol | string | `"TCP"` |  |
| log-forwarder.extraPorts[1].containerPort | int | `24224` |  |
| log-forwarder.extraPorts[1].name | string | `"forward"` |  |
| log-forwarder.extraPorts[1].port | int | `24224` |  |
| log-forwarder.extraPorts[1].protocol | string | `"TCP"` |  |
| log-forwarder.image.pullPolicy | string | `"IfNotPresent"` |  |
| log-forwarder.kind | string | `"Deployment"` |  |
| log-forwarder.labels."app.kubernetes.io/component" | string | `"logging"` |  |
| log-forwarder.labels."app.kubernetes.io/part-of" | string | `"spotfire"` |  |
| log-forwarder.podAnnotations."prometheus.io/path" | string | `"/api/v1/metrics/prometheus"` |  |
| log-forwarder.podAnnotations."prometheus.io/port" | string | `"2020"` |  |
| log-forwarder.podAnnotations."prometheus.io/scrape" | string | `"true"` |  |
| log-forwarder.podLabels."app.kubernetes.io/component" | string | `"logging"` |  |
| log-forwarder.podLabels."app.kubernetes.io/part-of" | string | `"spotfire"` |  |
| log-forwarder.rbac.create | bool | `false` | Specifies whether to create an rbac for the fluent-bit / log-forwarder. Setting this to `true` requires additional privileges in the Kubernetes cluster. |
| log-forwarder.service.labels."app.kubernetes.io/component" | string | `"logging"` |  |
| log-forwarder.service.labels."app.kubernetes.io/part-of" | string | `"spotfire"` |  |
| logging.logForwarderAddress | string | `""` | Specifies a logForwarderAddress. If left empty, then the default log-forwarder is used in the case where log-forwarder.enabled=true. Template. |
| logging.logLevel | string | `""` | The Spotfire Server log-level. Set to `debug`, `trace`, `minimal` or leave empty for info. |
| nodeSelector | object | `{}` |  |
| podAnnotations."prometheus.io/path" | string | `"/spotfire/metrics"` |  |
| podAnnotations."prometheus.io/port" | string | `"9080"` |  |
| podAnnotations."prometheus.io/scrape" | string | `"true"` |  |
| podSecurityContext | object | `{}` | The Pod securityContext setting applies to all of the containers inside the Pod. More info: `kubectl explain deployment.spec.template.spec.securityContext` |
| readinessProbe.enabled | bool | `false` |  |
| replicaCount | int | `1` | The number of Spotfire Server containers. |
| resources | object | `{}` |  |
| securityContext | object | `{}` | The securityContext setting for spotfire-server container. Overrides any securityContext setting on the Pod level. More info: `kubectl explain deployment.spec.template.spec.containers.securityContext` |
| service.type | string | `"ClusterIP"` |  |
| serviceAccount.annotations | object | `{}` |  |
| serviceAccount.create | bool | `true` |  |
| serviceAccount.name | string | `""` |  |
| site | object | Spotfire Server joins the Default site. | Site settings. See https://docs.tibco.com/pub/spotfire_server/latest/doc/html/TIB_sfire_server_tsas_admin_help/server/topics/sites.html for more information. |
| site.name | string | `"Default"` | The name of the site that the Spotfire Server should belong to. The site must be created beforehand. See https://docs.tibco.com/pub/spotfire_server/latest/doc/html/TIB_sfire_server_tsas_admin_help/server/topics/create-site.html for more information. |
| site.publicAddress | string | `""` | The address that clients use for connecting to the system. It is also used for generating absolute URLs. |
| spotfireAdminPassword | string | `""` | The password to create for the Spotfire admin. If not provided, this password is automatically generated. Although possible, it is not recommended to change the user's password directly in the Spotfire adminstrative user interace because the password is reset to this value on every helm installation or upgrade. |
| spotfireAdminUsername | string | `"admin"` | The user to create for the Spotfire admin. |
| spotfireServerJava.extraJavaOpts | list | `[]` | Additional JAVA_OPTS for spotfire-server pods. |
| startupProbe.enabled | bool | `true` |  |
| startupProbe.failureThreshold | int | `30` |  |
| startupProbe.httpGet.path | string | `"/spotfire/rest/status/getStatus"` |  |
| startupProbe.httpGet.port | string | `"http"` |  |
| startupProbe.initialDelaySeconds | int | `60` |  |
| startupProbe.periodSeconds | int | `10` |  |
| tolerations | list | `[]` |  |
| toolPassword | string | `"toolpassword"` | The Spotfire config tool password to use for bootstrap.xml. If not provided, this password is automatically generated. |
| troubleshooting.jvm.heapDumpOnOutOfMemoryError.dumpPath | string | `"/opt/tibco/troubleshooting/jvm-heap-dumps"` | Define a path where the generated dump is exported. By default, this gets mounted in EmptyDir: {} internally, which survives container restarts. In case you want to persist troubleshooting information to an external location, you can override the default behaviour by specifying PVC in .Values.volumes.troubleshooting. |
| troubleshooting.jvm.heapDumpOnOutOfMemoryError.enabled | bool | `true` | Enable or disable for a heap dump in case of OutOfMemoryError. |
| volumes.certificates.existingClaim | string | `""` |  |
| volumes.customExt.existingClaim | string | `""` |  |
| volumes.deployments.existingClaim | string | `""` |  |
| volumes.libraryImportExport.existingClaim | string | `""` |  |
| volumes.libraryImportExport.persistentVolumeClaim.create | bool | `false` | If 'true', then a 'PersistentVolumeClaim' is created. |
| volumes.libraryImportExport.persistentVolumeClaim.resources | object | `{"requests":{"storage":"1Gi"}}` | Specifies the standard Kubernetes resource requests and/or limits for the libraryImportExport volume claims. |
| volumes.libraryImportExport.persistentVolumeClaim.storageClassName | string | `""` | Specifies the name of the 'StorageClass' to use for the libraryImportExport volume-claim. |
| volumes.libraryImportExport.persistentVolumeClaim.volumeName | string | `nil` | Specifies the name of the persistent volume to use for the libraryImportExport volume-claim. |
| volumes.troubleshooting.existingClaim | string | `""` | When 'persistentVolumeClaim.create' is 'false', then use this value to define an already-existing persistent volume claim |
| volumes.troubleshooting.persistentVolumeClaim.create | bool | `false` | If 'true', then a 'PersistentVolumeClaim' is created. |
| volumes.troubleshooting.persistentVolumeClaim.resources | object | `{"requests":{"storage":"2Gi"}}` | Specifies the standard K8s resource requests and/or limits for the volumes.troubleshooting claims. |
| volumes.troubleshooting.persistentVolumeClaim.storageClassName | string | `""` | Specifies the name of the 'StorageClass' that to use for the volumes.troubleshooting-claim. |
| volumes.troubleshooting.persistentVolumeClaim.volumeName | string | `nil` | Specifies the name of the persistent volume to use for the volumes.troubleshooting-claim. |

